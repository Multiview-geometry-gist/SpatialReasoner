# SpatialReasoner Data Format Examples

ì´ ë””ë ‰í† ë¦¬ëŠ” SpatialReasonerì˜ í˜„ì¬ ë°ì´í„° í˜•ì‹ê³¼ ì œì•ˆëœ ê°œì„ ì‚¬í•­ì„ ë³´ì—¬ì£¼ëŠ” ì˜ˆì‹œë¥¼ í¬í•¨í•©ë‹ˆë‹¤.

## ğŸ“ íŒŒì¼ êµ¬ì¡°

```
examples/
â”œâ”€â”€ README.md                           # ì´ íŒŒì¼
â”œâ”€â”€ sample_data/                        # ìƒ˜í”Œ ì´ë¯¸ì§€ë“¤
â”‚   â”œâ”€â”€ sample_001_car_person.jpg      # ìë™ì°¨ì™€ ì‚¬ëŒ ì¥ë©´
â”‚   â”œâ”€â”€ sample_002_table_chair.jpg     # í…Œì´ë¸”ê³¼ ì˜ì ì¥ë©´
â”‚   â””â”€â”€ sample_003_box_sphere.jpg      # ë°•ìŠ¤ì™€ êµ¬ ì¥ë©´ (íšŒì „ ì¤‘ì‹¬)
â”œâ”€â”€ current_data_format.json           # í˜„ì¬ SpatialReasoner ë°ì´í„° í˜•ì‹
â”œâ”€â”€ proposed_quaternion_format.json    # ì œì•ˆ: ì¿¼í„°ë‹ˆì•ˆ íšŒì „ í‘œí˜„ (Phase 1)
â””â”€â”€ proposed_multiview_format.json     # ì œì•ˆ: ë‹¤ì¤‘ ì‹œì  + ì¿¼í„°ë‹ˆì•ˆ (Phase 2)
```

## ğŸ¯ ë°ì´í„° í˜•ì‹ ë¹„êµ

### 1. í˜„ì¬ í˜•ì‹ (Current)

**íŒŒì¼**: `current_data_format.json`

**íšŒì „ í‘œí˜„**: ë°©í–¥ ë²¡í„° (front_direction, left_direction)

```json
{
  "objects": [
    {
      "label": "car",
      "location_xyz": [0.0, 0.8, 8.5],
      "front_direction": [0.998, 0.0, 0.062],
      "left_direction": [-0.062, 0.0, 0.998]
    }
  ]
}
```

**ì¥ì **:
- ì§ê´€ì ìœ¼ë¡œ ì´í•´í•˜ê¸° ì‰¬ì›€
- êµ¬í˜„ì´ ê°„ë‹¨

**ë‹¨ì **:
- ë‘ ë²¡í„°ê°€ ë…ë¦½ì ì´ë¼ ì •ê·œì§êµì„± ë³´ì¥ í•„ìš”
- ë³´ê°„(interpolation) ì‹œ ë¶€ìì—°ìŠ¤ëŸ¬ì›€
- ë°©í–¥ ê¸°ë°˜ ì§ˆë¬¸ì— ëŒ€í•œ ê³„ì‚°ì´ ë³µì¡

---

### 2. Phase 1: ì¿¼í„°ë‹ˆì•ˆ í‘œí˜„

**íŒŒì¼**: `proposed_quaternion_format.json`

**íšŒì „ í‘œí˜„**: ë‹¨ìœ„ ì¿¼í„°ë‹ˆì•ˆ (w, x, y, z)

```json
{
  "objects": [
    {
      "label": "car",
      "location_xyz": [0.0, 0.8, 8.5],
      "rotation_quaternion": {
        "w": 0.9990,
        "x": 0.0000,
        "y": 0.0436,
        "z": 0.0000,
        "note": "5ë„ Yì¶• íšŒì „"
      }
    }
  ]
}
```

**ì¥ì **:
- âœ… **Gimbal lock ë¬¸ì œ í•´ê²°**
- âœ… **ì—°ì†ì ì¸ íšŒì „ í‘œí˜„** (ë³´ê°„ ê°€ëŠ¥)
- âœ… **ë‹¨ì¼ ì œì•½ ì¡°ê±´** (||q|| = 1)
- âœ… **íš¨ìœ¨ì ì¸ ê°ë„ ê³„ì‚°**

**ìƒˆë¡œìš´ ì§ˆë¬¸ ìœ í˜•**:
```json
{
  "question": "What is the angular difference between the box and sphere in degrees?",
  "answer_cot": "<think>\n...\nÎ¸ = 2 * arccos(w) = 30Â°\n</think><answer>A</answer>"
}
```

---

### 3. Phase 2: ë‹¤ì¤‘ ì‹œì  + ì¿¼í„°ë‹ˆì•ˆ

**íŒŒì¼**: `proposed_multiview_format.json`

**ì¶”ê°€ ìš”ì†Œ**:
- ë‹¤ì¤‘ ì´ë¯¸ì§€ (ì¤‘ì•™, ì¢Œ 5ë„, ìš° 5ë„)
- Depth map
- ì¹´ë©”ë¼ ë‚´ë¶€ íŒŒë¼ë¯¸í„°

```json
{
  "image_filenames": [
    "sample_001_car_person.jpg",          // ì¤‘ì•™ ì‹œì 
    "sample_001_car_person_left5deg.jpg",  // ì¢Œ 5ë„ ì‹œì 
    "sample_001_car_person_right5deg.jpg"  // ìš° 5ë„ ì‹œì 
  ],
  "depth_map": "sample_001_depth.npy",
  "viewpoint_angles": [0, -5, 5]
}
```

**í•™ìŠµ ì „ëµ**:
- í•™ìŠµ ì‹œ: 3ê°œ ì‹œì  ì¤‘ **ëœë¤ ì„ íƒ**
- ì¶”ë¡  ì‹œ: **ì¤‘ì•™ ì‹œì ë§Œ** ì‚¬ìš©

**ì¥ì **:
- 3D êµ¬ì¡°ì— ëŒ€í•œ ë” ë‚˜ì€ ì´í•´
- ê¹Šì´ ëª¨í˜¸ì„± ê°ì†Œ
- ê±°ë¦¬ ì¶”ì • ì •í™•ë„ í–¥ìƒ

---

## ğŸ–¼ï¸ ìƒ˜í”Œ ì´ë¯¸ì§€ ì„¤ëª…

### Sample 001: Car and Person
![car_person](sample_data/sample_001_car_person.jpg)

**íŠ¹ì§•**:
- ìë™ì°¨ê°€ ì‚¬ëŒë³´ë‹¤ ì¹´ë©”ë¼ì— ê°€ê¹Œì›€ (ê¹Šì´ ì°¨ì´ ~4m)
- ìë™ì°¨ê°€ ì•½ê°„ ìš°ì¸¡ì„ í–¥í•¨ (~5ë„ íšŒì „)

**ì§ˆë¬¸ ì˜ˆì‹œ**:
- "Is the car closer to camera than the person?" â†’ **Yes**
- "Is the car facing towards the person?" â†’ **Partially**

---

### Sample 002: Table and Chair
![table_chair](sample_data/sample_002_table_chair.jpg)

**íŠ¹ì§•**:
- ì˜ìê°€ í…Œì´ë¸”ì˜ ì™¼ìª½ì— ìœ„ì¹˜
- ì˜ìê°€ í…Œì´ë¸”ì„ í–¥í•´ 45ë„ íšŒì „

**ì§ˆë¬¸ ì˜ˆì‹œ**:
- "Is the chair on the left of the table?" â†’ **Yes**
- "Is the chair facing the table?" â†’ **Yes** (45ë„ íšŒì „ì´ì§€ë§Œ í–¥í•˜ê³  ìˆìŒ)

---

### Sample 003: Box and Sphere
![box_sphere](sample_data/sample_003_box_sphere.jpg)

**íŠ¹ì§•**:
- ë°•ìŠ¤ê°€ 30ë„ íšŒì „ë˜ì–´ ìˆìŒ
- êµ¬ëŠ” íšŒì „ ì—†ìŒ

**ì§ˆë¬¸ ì˜ˆì‹œ** (ì¿¼í„°ë‹ˆì•ˆ í™œìš©):
- "What is the angular difference?" â†’ **30 degrees**
- "Is the box facing the sphere?" â†’ **Yes** (ê°ë„ ê³„ì‚° ê¸°ë°˜)

---

## ğŸ“Š ë°ì´í„° í†µê³„ (ì˜ˆìƒ)

| í•­ëª© | Current | Phase 1 (Quaternion) | Phase 2 (Multi-view) |
|------|---------|----------------------|----------------------|
| **ì´ë¯¸ì§€ ìˆ˜** | 24,000 | 24,000 | 24,000 |
| **ì´ë¯¸ì§€ íŒŒì¼ ìˆ˜** | 24,000 | 24,000 | 72,000 (Ã—3) |
| **ì§ˆë¬¸ ì¹´í…Œê³ ë¦¬** | 9ê°œ | 12ê°œ (+3) | 12ê°œ |
| **íšŒì „ ì§ˆë¬¸ ë¹„ìœ¨** | ~20% | ~35% | ~35% |
| **ì €ì¥ ê³µê°„** | ~15GB | ~15GB | ~45GB |

---

## ğŸ”§ ë°ì´í„° ë¡œë”© ì˜ˆì‹œ ì½”ë“œ

### Current Format
```python
from PIL import Image
import json

# Load metadata
with open('current_data_format.json') as f:
    data = json.load(f)

sample = data[0]
image = Image.open(sample['image_filename'])

# Access object info
car = sample['objects'][0]
front_dir = car['front_direction']  # [0.998, 0.0, 0.062]
```

### Proposed Quaternion Format
```python
import numpy as np
from scipy.spatial.transform import Rotation

# Load sample
sample = data[0]
car = sample['objects'][0]

# Get quaternion
quat = car['rotation_quaternion']
q = np.array([quat['w'], quat['x'], quat['y'], quat['z']])

# Convert to rotation matrix if needed
rot = Rotation.from_quat([quat['x'], quat['y'], quat['z'], quat['w']])
front_vector = rot.apply([0, 0, 1])  # Apply to forward vector
```

### Proposed Multi-view Format
```python
import random

sample = data[0]

# Training: random view selection
if training:
    view_idx = random.choice([0, 1, 2])
else:
    view_idx = 0  # Inference: center view only

image_path = sample['image_filenames'][view_idx]
image = Image.open(image_path)
```

---

## ğŸ“ CoT (Chain-of-Thought) ë‹µë³€ í˜•ì‹

ëª¨ë“  ìƒ˜í”Œì€ `<think>...</think><answer>...</answer>` í˜•ì‹ì„ ë”°ë¦…ë‹ˆë‹¤.

### ì˜ˆì‹œ: í˜„ì¬ í˜•ì‹ (ë°©í–¥ ë²¡í„°)
```xml
<think>
Step 1: Get front direction of box
- Front direction: (0.866, 0.0, 0.5)

Step 2: Calculate vector to target
- Vector to sphere: (0.996, 0.030, 0.091)

Step 3: Compute angle
- Cosine similarity: 0.909
- Angle: 24.5 degrees
</think><answer>A</answer>
```

### ì˜ˆì‹œ: ì¿¼í„°ë‹ˆì•ˆ í˜•ì‹
```xml
<think>
Step 1: Get quaternions
- Box: (0.9659, 0.0, 0.2588, 0.0)
- Sphere: (1.0, 0.0, 0.0, 0.0)

Step 2: Calculate relative rotation
- q_diff = q1 * q2^-1

Step 3: Extract angle
- Î¸ = 2 * arccos(0.9659) = 30.0Â°
</think><answer>A</answer>
```

---

## ğŸ“ˆ ê¸°ëŒ€ íš¨ê³¼

### Phase 1 (Quaternion)
- íšŒì „ ê´€ë ¨ ì§ˆë¬¸ ì •í™•ë„: **+5~10%**
- Gimbal lock ë¬¸ì œ: **ì™„ì „ í•´ê²°**
- ê°ë„ ê³„ì‚° ì˜¤ì°¨: **-30%**

### Phase 2 (Multi-view)
- ê¹Šì´ ì¶”ì • ì˜¤ì°¨: **-20~30%**
- ì „ì²´ ì •í™•ë„: **+3~8%**
- ê±°ë¦¬ ì§ˆë¬¸ ì •í™•ë„: **+10~15%**

---

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„

1. **Phase 1 êµ¬í˜„ (ì¶”ì²œ)**
   - [ ] 6DoF pose estimator í†µí•©
   - [ ] ì¿¼í„°ë‹ˆì•ˆ ë³€í™˜ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
   - [ ] Reward í•¨ìˆ˜ í™•ì¥
   - [ ] ì§ˆë¬¸ ìƒì„± í…œí”Œë¦¿ ì‘ì„±

2. **Phase 2 ê³ ë ¤ (Phase 1 ê²°ê³¼ í›„)**
   - [ ] Depth map ìƒì„± (Depth Anything v2)
   - [ ] ì‹œì  í•©ì„± íŒŒì´í”„ë¼ì¸
   - [ ] í’ˆì§ˆ ê²€ì¦ ë©”íŠ¸ë¦­

---

## ğŸ“ ì°¸ê³  ìë£Œ

- **ì¿¼í„°ë‹ˆì•ˆ ìˆ˜í•™**: https://en.wikipedia.org/wiki/Quaternion
- **SpatialReasoner ë…¼ë¬¸**: https://spatial-reasoner.github.io/
- **Depth Anything v2**: https://github.com/DepthAnything/Depth-Anything-V2
- **SAM2**: https://github.com/facebookresearch/segment-anything-2

---

**ì§ˆë¬¸ì´ë‚˜ í”¼ë“œë°±**ì€ GitHub Issuesì— ë‚¨ê²¨ì£¼ì„¸ìš”!
